{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformations'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\BearKS\\Image-Augmentation-Web\\backend\\Augment.ipynb Cell 1'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BearKS/Image-Augmentation-Web/backend/Augment.ipynb#ch0000000?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models, layers, datasets, utils, backend, optimizers, initializers\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BearKS/Image-Augmentation-Web/backend/Augment.ipynb#ch0000000?line=7'>8</a>\u001b[0m backend\u001b[39m.\u001b[39mset_session(session)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/BearKS/Image-Augmentation-Web/backend/Augment.ipynb#ch0000000?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformations\u001b[39;00m \u001b[39mimport\u001b[39;00m get_transformations\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BearKS/Image-Augmentation-Web/backend/Augment.ipynb#ch0000000?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mPIL\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mImage\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BearKS/Image-Augmentation-Web/backend/Augment.ipynb#ch0000000?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformations'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "from keras import models, layers, datasets, utils, backend, optimizers, initializers\n",
    "backend.set_session(session)\n",
    "from transformations import get_transformations\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# datasets in the AutoAugment paper:\n",
    "# CIFAR-10, CIFAR-100, SVHN, and ImageNet\n",
    "# SVHN = http://ufldl.stanford.edu/housenumbers/\n",
    "\n",
    "def get_dataset(dataset, reduced):\n",
    "    if dataset == 'cifar10':\n",
    "        (Xtr, ytr), (Xts, yts) = datasets.cifar10.load_data()\n",
    "    elif dataset == 'cifar100':\n",
    "        (Xtr, ytr), (Xts, yts) = datasets.cifar100.load_data()\n",
    "    else:\n",
    "        raise Exception('Unknown dataset %s' % dataset)\n",
    "    if reduced:\n",
    "        ix = np.random.choice(len(Xtr), 4000, False)\n",
    "        Xtr = Xtr[ix]\n",
    "        ytr = ytr[ix]\n",
    "    ytr = utils.to_categorical(ytr)\n",
    "    yts = utils.to_categorical(yts)\n",
    "    return (Xtr, ytr), (Xts, yts)\n",
    "\n",
    "(Xtr, ytr), (Xts, yts) = get_dataset('cifar10', True)\n",
    "transformations = get_transformations(Xtr)\n",
    "\n",
    "# Experiment parameters\n",
    "\n",
    "LSTM_UNITS = 100\n",
    "\n",
    "SUBPOLICIES = 5\n",
    "SUBPOLICY_OPS = 2\n",
    "\n",
    "OP_TYPES = 16\n",
    "OP_PROBS = 11\n",
    "OP_MAGNITUDES = 10\n",
    "\n",
    "CHILD_BATCH_SIZE = 128\n",
    "CHILD_BATCHES = len(Xtr) // CHILD_BATCH_SIZE\n",
    "CHILD_EPOCHS = 120\n",
    "CONTROLLER_EPOCHS = 500 # 15000 or 20000\n",
    "\n",
    "class Operation:\n",
    "    def __init__(self, types_softmax, probs_softmax, magnitudes_softmax, argmax=False):\n",
    "        # Ekin Dogus says he sampled the softmaxes, and has not used argmax\n",
    "        # We might still want to use argmax=True for the last predictions, to ensure\n",
    "        # the best solutions are chosen and make it deterministic.\n",
    "        if argmax:\n",
    "            self.type = types_softmax.argmax()\n",
    "            t = transformations[self.type]\n",
    "            self.prob = probs_softmax.argmax() / (OP_PROBS-1)\n",
    "            m = magnitudes_softmax.argmax() / (OP_MAGNITUDES-1)\n",
    "            self.magnitude = m*(t[2]-t[1]) + t[1]\n",
    "        else:\n",
    "            self.type = np.random.choice(OP_TYPES, p=types_softmax)\n",
    "            t = transformations[self.type]\n",
    "            self.prob = np.random.choice(np.linspace(0, 1, OP_PROBS), p=probs_softmax)\n",
    "            self.magnitude = np.random.choice(np.linspace(t[1], t[2], OP_MAGNITUDES), p=magnitudes_softmax)\n",
    "        self.transformation = t[0]\n",
    "\n",
    "    def __call__(self, X):\n",
    "        _X = []\n",
    "        for x in X:\n",
    "            if np.random.rand() < self.prob:\n",
    "                x = PIL.Image.fromarray(x)\n",
    "                x = self.transformation(x, self.magnitude)\n",
    "            _X.append(np.array(x))\n",
    "        return np.array(_X)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Operation %2d (P=%.3f, M=%.3f)' % (self.type, self.prob, self.magnitude)\n",
    "\n",
    "class Subpolicy:\n",
    "    def __init__(self, *operations):\n",
    "        self.operations = operations\n",
    "\n",
    "    def __call__(self, X):\n",
    "        for op in self.operations:\n",
    "            X = op(X)\n",
    "        return X\n",
    "\n",
    "    def __str__(self):\n",
    "        ret = ''\n",
    "        for i, op in enumerate(self.operations):\n",
    "            ret += str(op)\n",
    "            if i < len(self.operations)-1:\n",
    "                ret += '\\n'\n",
    "        return ret\n",
    "\n",
    "class Controller:\n",
    "    def __init__(self):\n",
    "        self.model = self.create_model()\n",
    "        self.scale = tf.placeholder(tf.float32, ())\n",
    "        self.grads = tf.gradients(self.model.outputs, self.model.trainable_weights)\n",
    "        # negative for gradient ascent\n",
    "        self.grads = [g * (-self.scale) for g in self.grads]\n",
    "        self.grads = zip(self.grads, self.model.trainable_weights)\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(0.00035).apply_gradients(self.grads)\n",
    "\n",
    "    def create_model(self):\n",
    "        # Implementation note: Keras requires an input. I create an input and then feed\n",
    "        # zeros to the network. Ugly, but it's the same as disabling those weights.\n",
    "        # Furthermore, Keras LSTM input=output, so we cannot produce more than SUBPOLICIES\n",
    "        # outputs. This is not desirable, since the paper produces 25 subpolicies in the\n",
    "        # end.\n",
    "        input_layer = layers.Input(shape=(SUBPOLICIES, 1))\n",
    "        init = initializers.RandomUniform(-0.1, 0.1)\n",
    "        lstm_layer = layers.LSTM(\n",
    "            LSTM_UNITS, recurrent_initializer=init, return_sequences=True,\n",
    "            name='controller')(input_layer)\n",
    "        outputs = []\n",
    "        for i in range(SUBPOLICY_OPS):\n",
    "            name = 'op%d-' % (i+1)\n",
    "            outputs += [\n",
    "                layers.Dense(OP_TYPES, activation='softmax', name=name + 't')(lstm_layer),\n",
    "                layers.Dense(OP_PROBS, activation='softmax', name=name + 'p')(lstm_layer),\n",
    "                layers.Dense(OP_MAGNITUDES, activation='softmax', name=name + 'm')(lstm_layer),\n",
    "            ]\n",
    "        return models.Model(input_layer, outputs)\n",
    "\n",
    "    def fit(self, mem_softmaxes, mem_accuracies):\n",
    "        session = backend.get_session()\n",
    "        min_acc = np.min(mem_accuracies)\n",
    "        max_acc = np.max(mem_accuracies)\n",
    "        dummy_input = np.zeros((1, SUBPOLICIES, 1))\n",
    "        dict_input = {self.model.input: dummy_input}\n",
    "        # FIXME: the paper does mini-batches (10)\n",
    "        for softmaxes, acc in zip(mem_softmaxes, mem_accuracies):\n",
    "            scale = (acc-min_acc) / (max_acc-min_acc)\n",
    "            dict_outputs = {_output: s for _output, s in zip(self.model.outputs, softmaxes)}\n",
    "            dict_scales = {self.scale: scale}\n",
    "            session.run(self.optimizer, feed_dict={**dict_outputs, **dict_scales, **dict_input})\n",
    "        return self\n",
    "\n",
    "    def predict(self, size):\n",
    "        dummy_input = np.zeros((1, size, 1), np.float32)\n",
    "        softmaxes = self.model.predict(dummy_input)\n",
    "        # convert softmaxes into subpolicies\n",
    "        subpolicies = []\n",
    "        for i in range(SUBPOLICIES):\n",
    "            operations = []\n",
    "            for j in range(SUBPOLICY_OPS):\n",
    "                op = softmaxes[j*3:(j+1)*3]\n",
    "                op = [o[0, i, :] for o in op]\n",
    "                operations.append(Operation(*op))\n",
    "            subpolicies.append(Subpolicy(*operations))\n",
    "        return softmaxes, subpolicies\n",
    "\n",
    "# generator\n",
    "def autoaugment(subpolicies, X, y):\n",
    "    while True:\n",
    "        ix = np.arange(len(X))\n",
    "        np.random.shuffle(ix)\n",
    "        for i in range(CHILD_BATCHES):\n",
    "            _ix = ix[i*CHILD_BATCH_SIZE:(i+1)*CHILD_BATCH_SIZE]\n",
    "            _X = X[_ix]\n",
    "            _y = y[_ix]\n",
    "            subpolicy = np.random.choice(subpolicies)\n",
    "            _X = subpolicy(_X)\n",
    "            _X = _X.astype(np.float32) / 255\n",
    "            yield _X, _y\n",
    "\n",
    "class Child:\n",
    "    # architecture from: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "    def __init__(self, input_shape):\n",
    "        self.model = self.create_model(input_shape)\n",
    "        optimizer = optimizers.SGD(decay=1e-4)\n",
    "        self.model.compile(optimizer, 'categorical_crossentropy', ['accuracy'])\n",
    "\n",
    "    def create_model(self, input_shape):\n",
    "        x = input_layer = layers.Input(shape=input_shape)\n",
    "        x = layers.Conv2D(32, 3, activation='relu')(x)\n",
    "        x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(10, activation='softmax')(x)\n",
    "        return models.Model(input_layer, x)\n",
    "\n",
    "    def fit(self, subpolicies, X, y):\n",
    "        gen = autoaugment(subpolicies, X, y)\n",
    "        self.model.fit_generator(\n",
    "            gen, CHILD_BATCHES, CHILD_EPOCHS, verbose=0, use_multiprocessing=True)\n",
    "        return self\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        return self.model.evaluate(X, y, verbose=0)[1]\n",
    "\n",
    "mem_softmaxes = []\n",
    "mem_accuracies = []\n",
    "\n",
    "controller = Controller()\n",
    "\n",
    "for epoch in range(CONTROLLER_EPOCHS):\n",
    "    print('Controller: Epoch %d / %d' % (epoch+1, CONTROLLER_EPOCHS))\n",
    "\n",
    "    softmaxes, subpolicies = controller.predict(SUBPOLICIES)\n",
    "    for i, subpolicy in enumerate(subpolicies):\n",
    "        print('# Sub-policy %d' % (i+1))\n",
    "        print(subpolicy)\n",
    "    mem_softmaxes.append(softmaxes)\n",
    "\n",
    "    child = Child(Xtr.shape[1:])\n",
    "    tic = time.time()\n",
    "    child.fit(subpolicies, Xtr, ytr)\n",
    "    toc = time.time()\n",
    "    accuracy = child.evaluate(Xts, yts)\n",
    "    print('-> Child accuracy: %.3f (elaspsed time: %ds)' % (accuracy, (toc-tic)))\n",
    "    mem_accuracies.append(accuracy)\n",
    "\n",
    "    if len(mem_softmaxes) > 5:\n",
    "        # ricardo: I let some epochs pass, so that the normalization is more robust\n",
    "        controller.fit(mem_softmaxes, mem_accuracies)\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print('Best policies found:')\n",
    "print()\n",
    "_, subpolicies = controller.predict(25)\n",
    "for i, subpolicy in enumerate(subpolicies):\n",
    "    print('# Subpolicy %d' % (i+1))\n",
    "    print(subpolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
